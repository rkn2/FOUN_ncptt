{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Interactive Diagnostics - From Concepts to Code\n",
    "\n",
    "**Grant:** NCPTT Data-Driven Heritage Preservation\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Goal: Accessible Data Science for Preservation\n",
    "\n",
    "This tutorial is designed for **all skill levels**:\n",
    "1.  **No-Code Explorer:** Use sliders and buttons to understand the concepts.\n",
    "2.  **Low-Code Student:** Read the explanations to understand *how* it works.\n",
    "3.  **Code-Curious:** Expand the \"Under the Hood\" sections to see the Python implementation.\n",
    "\n",
    "**We will explore:**\n",
    "-   **Factor Analysis:** Grouping hidden patterns of damage.\n",
    "-   **Machine Learning (Elastic Net):** Finding out what *really* causes degradation using advanced regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Install Libraries\n",
    "**Run this cell first to install the required analysis tools.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install factor_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc2 Google Colab: How to Load Data\n",
    "**If you are running this in Google Colab, you must upload the data file first.**\n",
    "\n",
    "1.  Download `synthetic_adobe_data.csv` from the GitHub repository to your computer.\n",
    "2.  In Colab, click the **Folder Icon** \ud83d\udcc1 on the left sidebar.\n",
    "3.  Click the **Upload Icon** (page with an upward arrow).\n",
    "4.  Select `synthetic_adobe_data.csv` from your computer.\n",
    "5.  Wait for the upload to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udee0\ufe0f Setup (Run this once)\n",
    "Press `Shift + Enter` on the cell below to load our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Style settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv('synthetic_adobe_data.csv')\n",
    "    # Preprocess\n",
    "    cols_to_drop = ['Wall ID', 'Image Name', 'Notes', 'Date', 'Reviewer', 'Wall Rank', 'Section ID']\n",
    "    df_clean = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "    df_numeric = df_clean.select_dtypes(include=[np.number]).dropna(axis=1, how='all')\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "    print(\"\u2705 Data Loaded Successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\u26a0\ufe0f Data file not found. Please run 'generate_synthetic_data.py' first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udd0d Part 1: Data Explorer (No-Code)\n",
    "\n",
    "Before analyzing, we must look at our data. \n",
    "\n",
    "**Interactive Task:**\n",
    "1.  Select a variable from the dropdown.\n",
    "2.  Look at the histogram. Is the data spread out? Clumped together?\n",
    "3.  *Preservation Insight:* If \"Cap Deterioration\" is mostly high numbers (4-5), we have a serious roofing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INTERACTIVE WIDGET CODE ---\n",
    "def plot_distribution(column):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df_imputed[column], kde=True, color='teal')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel('Score / Value')\n",
    "    plt.ylabel('Count of Walls')\n",
    "    plt.show()\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=sorted(df_imputed.columns),\n",
    "    value='Total Scr',\n",
    "    description='Variable:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "widgets.interactive(plot_distribution, column=dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83e\udde9 Part 2: Factor Analysis (Low-Code)\n",
    "\n",
    "**The Concept:** \n",
    "Imagine you have 30 different symptoms. A doctor groups them into 1 diagnosis (e.g., \"Flu\"). \n",
    "**Factor Analysis** does the same for buildings. It groups 30 damage types into a few \"Latent Vulnerabilities.\"\n",
    "\n",
    "**Interactive Task:**\n",
    "1.  Use the slider to change the **Number of Factors**.\n",
    "2.  Watch the heatmap change.\n",
    "3.  **Goal:** Find the number where the groups look distinct (dark red blocks) and make physical sense.\n",
    "    *   *Hint: Try 3 factors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INTERACTIVE FACTOR ANALYSIS ---\n",
    "def run_interactive_fa(n_factors=3):\n",
    "    fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')\n",
    "    fa.fit(df_imputed)\n",
    "    \n",
    "    # Get loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        fa.loadings_, \n",
    "        index=df_imputed.columns, \n",
    "        columns=[f'Factor {i+1}' for i in range(n_factors)]\n",
    "    )\n",
    "    \n",
    "    # Filter for visibility (hide weak connections)\n",
    "    significant = loadings[loadings.abs().max(axis=1) > 0.4]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(significant, annot=True, fmt='.2f', cmap='RdBu_r', center=0, vmin=-1, vmax=1)\n",
    "    plt.title(f'Factor Analysis with {n_factors} Factors')\n",
    "    plt.show()\n",
    "\n",
    "slider_fa = widgets.IntSlider(value=3, min=2, max=6, step=1, description='Factors:')\n",
    "widgets.interactive(run_interactive_fa, n_factors=slider_fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83e\udde0 Reflection\n",
    "If you selected **3 Factors**, you likely saw:\n",
    "1.  **Factor 1:** Sill Deterioration (Windows)\n",
    "2.  **Factor 2:** Surface Coating Degradation (Lintels & Plaster)\n",
    "3.  **Factor 3:** Structural Instability (Cracking & Out-of-Plane)\n",
    "\n",
    "This tells us we have three main \"Diseases\" attacking the fort, matching the findings in the manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83e\udd16 Part 3: Machine Learning (Low-Code)\n",
    "\n",
    "**The Concept:**\n",
    "We want to know: *What causes the MOST damage?*\n",
    "We use **Elastic Net Regression**, which is a smart way to fit a line through data while ignoring noise. It's better than simple regression because it handles small datasets well and removes unimportant variables (sets them to zero).\n",
    "\n",
    "**Interactive Task:**\n",
    "1.  **Alpha (Regularization):** Controls how simple the model should be. Higher = Simpler (more zeros).\n",
    "2.  **L1 Ratio:** Mixes two types of penalties. 1.0 = Lasso (removes variables aggressively), 0.0 = Ridge (shrinks all variables).\n",
    "3.  Observe which features stay at the top. Are they consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INTERACTIVE ELASTIC NET ---\n",
    "def run_interactive_en(alpha=1.0, l1_ratio=0.5):\n",
    "    X = df_imputed.drop(columns=['Total Scr'])\n",
    "    y = df_imputed['Total Scr']\n",
    "    \n",
    "    # Scale data (important for Elastic Net)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Fit Model\n",
    "    en = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=10000)\n",
    "    en.fit(X_scaled, y)\n",
    "    \n",
    "    # Get coefficients\n",
    "    coefs = pd.Series(en.coef_, index=X.columns)\n",
    "    \n",
    "    # Sort by absolute value\n",
    "    importances = coefs.abs().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=importances.values, y=importances.index, palette='viridis')\n",
    "    plt.title(f'Top Predictors (Alpha={alpha}, L1 Ratio={l1_ratio})')\n",
    "    plt.xlabel('Coefficient Magnitude (Importance)')\n",
    "    plt.show()\n",
    "\n",
    "slider_alpha = widgets.FloatSlider(value=1.0, min=0.1, max=10.0, step=0.1, description='Alpha:')\n",
    "slider_l1 = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.1, description='L1 Ratio:')\n",
    "\n",
    "ui = widgets.HBox([slider_alpha, slider_l1])\n",
    "out = widgets.interactive_output(run_interactive_en, {'alpha': slider_alpha, 'l1_ratio': slider_l1})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udf93 Under the Hood (For the Code-Curious)\n",
    "\n",
    "How did we do that? Here is the raw Python code used in the widget above.\n",
    "\n",
    "```python\n",
    "# 1. Separate Predictors (X) and Target (y)\n",
    "X = df.drop(columns=['Total Scr'])\n",
    "y = df['Total Scr']\n",
    "\n",
    "# 2. Scale the Data (Crucial for Elastic Net)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Create the Model\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "\n",
    "# 4. Train the Model\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# 5. Get Importance Scores (Coefficients)\n",
    "scores = model.coef_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\ude4f Acknowledgements\n",
    "\n",
    "This material was developed under a grant from the **National Center for Preservation Technology and Training (NCPTT)**.\n",
    "\n",
    "*Data-Driven Heritage Preservation: Leveraging Machine Learning for Informed Adobe Conservation Strategies*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}